import os
import streamlit as st
import pandas as pd
import logging
from datetime import datetime
from typing import Dict, Any
import traceback 
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain.callbacks.base import BaseCallbackHandler
from time import sleep

# è¿½åŠ : CSS ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§æ³¨å…¥ã™ã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
CSS_PATH = "style_report.css"

def _inject_report_css():
    """style_report.css ãŒå­˜åœ¨ã™ã‚Œã°èª­ã¿è¾¼ã¿ã€<style> ã‚¿ã‚°ã¨ã—ã¦æŒ¿å…¥ã™ã‚‹"""
    if os.path.exists(CSS_PATH):
        try:
            with open(CSS_PATH, "r", encoding="utf-8") as f:
                css_content = f.read()
            st.markdown(f"<style>{css_content}</style>", unsafe_allow_html=True)
        except Exception as css_err:
            logging.warning(f"CSS èª­ã¿è¾¼ã¿ã«å¤±æ•—: {css_err}")
    else:
        logging.info(f"{CSS_PATH} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚CSS ã¯èª­ã¿è¾¼ã¾ã‚Œã¾ã›ã‚“ã€‚")

from langgraph.graph import StateGraph
from langchain_openai import ChatOpenAI
from langchain_community.callbacks.streamlit import StreamlitCallbackHandler
from langchain_core.prompts import PromptTemplate
from pydantic import BaseModel

# è‡ªä½œãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
import code_executor
from safety_checker import SafetyChecker
from code_executor import CodeExecutor
from llm_client import LLMClient
from pandas_dataframe_agent import create_pandas_dataframe_agent

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

###############################################################################
# Context and Utility Dataclasses
###############################################################################

class ConversationContext:
    """
    ä¸€é€£ã®åˆ†æã¨ãƒãƒ£ãƒƒãƒˆã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚
    """
    def __init__(self):
        self.user_prompt: str | None = None
        self.refined_prompt: str | None = None
        self.plan: pd.DataFrame | None = None
        self.prepare_results: Dict[str, pd.DataFrame] | None = None
        self.visualize_results: list[Dict[str, str]] | None = None
        self.report: str | None = None
        self.chat_history: list[Dict[str, str]] = []

    def get_full_context_as_string(self) -> str:
        """
        ä¿æŒã—ã¦ã„ã‚‹ã™ã¹ã¦ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å˜ä¸€ã®æ–‡å­—åˆ—ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã—ã¦è¿”ã™ã€‚
        """
        context_parts = []
        if self.user_prompt:
            context_parts.append(f"## ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®å…ƒã®ä¾é ¼å†…å®¹\n{self.user_prompt}")
        if self.refined_prompt:
            context_parts.append(f"## åˆ†ææ–¹é‡\n{self.refined_prompt}")
        if self.plan is not None and not self.plan.empty:
            context_parts.append(f"## å®Ÿè¡Œã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ãƒ—ãƒ©ãƒ³\n{self.plan.to_markdown()}")
        if self.prepare_results:
            prepared_dfs = ", ".join(self.prepare_results.keys())
            context_parts.append(f"## ã‚¿ã‚¹ã‚¯(prepare)ã§ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ä¸€è¦§\n{prepared_dfs}")
        if self.visualize_results:
            viz_tasks = "\n".join([f"- {res.get('task', 'N/A')}" for res in self.visualize_results])
            context_parts.append(f"## ã‚¿ã‚¹ã‚¯(visualize)ã®æ¦‚è¦\n{viz_tasks}")
        if self.report:
            context_parts.append(f"## ç”Ÿæˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆå…¨æ–‡\n{self.report}")

        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’æ™‚ç³»åˆ—ã§è¿½åŠ  (æœ€æ–°ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•ã¯å«ã‚ãªã„)
        history_str = "\n".join([f"### {msg['role']}\n{msg['content']}" for msg in self.chat_history[:-1]])
        if history_str:
            context_parts.append(f"## ã“ã‚Œã¾ã§ã®ãƒãƒ£ãƒƒãƒˆå±¥æ­´\n{history_str}")
            
        return "\n\n---\n\n".join(context_parts)

class Plan(BaseModel):
    category: str
    task: str
    input: list[str]
    output: str | None
    output_columns: list[str] | None

class ResponseFormatter(BaseModel):
    plans: list[Plan]

class RefinePromptFormat(BaseModel):
    need_clarification: bool
    questions: list[str] | None = None
    refined_prompt: str | None = None

###############################################################################
# Session Helpers                                                              
###############################################################################

def initialize_session_state(initial_df_dict: Dict[str, pd.DataFrame]):
    # ä¸€åº¦ã ã‘å®Ÿè¡Œã•ã‚Œã‚‹åˆæœŸåŒ–å‡¦ç†
    if "initialized" not in st.session_state:
        st.session_state.initial_df_dict = initial_df_dict
        st.session_state.safety_checker = SafetyChecker()
        st.session_state.code_executor = CodeExecutor(initial_df_dict)
        st.session_state.execution_history = []
        st.session_state.generated_codes = []
        st.session_state.generated_report = ""
        st.session_state.work_df_dict: Dict[str, pd.DataFrame] = {}
        st.session_state.initialized = True

    # ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½è¿½åŠ ã«ä¼´ã† stateã€‚å¤ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§ã‚‚ã‚¨ãƒ©ãƒ¼ã«ãªã‚‰ãªã„ã‚ˆã†ã«ã€å­˜åœ¨ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦åˆæœŸåŒ–ã™ã‚‹ã€‚
    if "conversation_context" not in st.session_state:
        st.session_state.conversation_context = ConversationContext()
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "app_status" not in st.session_state:
        st.session_state.app_status = "initial"
    if "clarification_history" not in st.session_state:
        st.session_state.clarification_history = []


def log_execution(prompt: str, code: str, success: bool, error: str | None = None):
    st.session_state.execution_history.append(
        {
            "timestamp": datetime.now(),
            "prompt": prompt,
            "code": code,
            "success": success,
            "error": error,
        }
    )


# DFåã¨æ—¥æœ¬èªè§£èª¬ã®ãƒãƒƒãƒ”ãƒ³ã‚°
DF_DESCRIPTIONS = {
    'balances': 'å£åº§æ®‹é«˜ãƒ‡ãƒ¼ã‚¿',
    'transactions': 'å–å¼•å±¥æ­´ãƒ‡ãƒ¼ã‚¿',
    'fx_rates': 'ç‚ºæ›¿ãƒ¬ãƒ¼ãƒˆ',
    'payables': 'è²·æ›é‡‘ãƒ‡ãƒ¼ã‚¿',
    'receivables': 'å£²æ›é‡‘ãƒ‡ãƒ¼ã‚¿',
    'loans': 'å€Ÿå…¥é‡‘ãƒ‡ãƒ¼ã‚¿',
    'investments': 'æŠ•è³‡ãƒ‡ãƒ¼ã‚¿',
    'derivatives': 'ãƒ‡ãƒªãƒãƒ†ã‚£ãƒ–ãƒ‡ãƒ¼ã‚¿',
}


def get_dataframe_info(df_list: list[dict[str, pd.DataFrame]]) -> str:
    info = ""
    for df_info in df_list:
        df_name = df_info["input_name"]
        description = DF_DESCRIPTIONS.get(df_name, "èª¬æ˜ãªã—")
        info += f"input_name: {df_name} ({description})\n"
        if isinstance(df_info["input_df"], pd.DataFrame):
            info += f"Shape: {df_info['input_df'].shape}\n"
            info += f"Columns: {', '.join(df_info['input_df'].columns)}\n"
            info += "Data types:\n"
            for col, dtype in df_info["input_df"].dtypes.items():
                info += f"  {col}: {dtype}\n"
        else:
            info += f"len(input_df): {len(df_info['input_df'])}\n"
        info += "\n"
    return info


def replace_df_references(code: str, df_names: list[str], df_dict: Dict[str, pd.DataFrame] | None = None) -> str:
    """ã‚³ãƒ¼ãƒ‰ä¸­ã® DataFrame å¤‰æ•°åã‚’ st.session_state.work_df_dict å‚ç…§ã«ç½®ãæ›ãˆã‚‹ã€‚

    Parameters
    ----------
    code: str
        å¤‰æ›å¯¾è±¡ã®ã‚³ãƒ¼ãƒ‰ã€‚
    df_names: list[str]
        ç½®ãæ›ãˆå¯¾è±¡ã¨ãªã‚‹ DataFrame åãƒªã‚¹ãƒˆã€‚
    df_dict: dict[str, pd.DataFrame] | None
        å‚ç…§å¯èƒ½ãª DataFrame è¾æ›¸ã€‚ã‚µãƒ–ã‚¹ãƒ¬ãƒƒãƒ‰ã§ã¯ session_state ã‚’ä½¿ã‚ãšã«
        ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’æ¸¡ã™ãŸã‚ã€ã“ã®å¼•æ•°ã‚’ä½¿ç”¨ã™ã‚‹ã€‚
    """
    import io, tokenize
    from tokenize import TokenInfo

    df_set = set(df_names)

    # ã‚µãƒ–ã‚¹ãƒ¬ãƒƒãƒ‰ã§ã¯ session_state ã‚’è§¦ã‚‰ãªã„
    if df_dict is None:
        df_dict = st.session_state.get("work_df_dict", {})

    replace_map = {
        name: f"st.session_state.work_df_dict['{name}']" for name in df_set if name in df_dict
    }

    result_tokens: list[TokenInfo] = []
    sio = io.StringIO(code)

    for tok in tokenize.generate_tokens(sio.readline):
        if tok.type == tokenize.NAME and tok.string in replace_map:
            tok = TokenInfo(tok.type, replace_map[tok.string], tok.start, tok.end, tok.line)
        result_tokens.append(tok)

    return tokenize.untokenize(result_tokens)

# ---------------------------------------------------------------------------
# Callback handler for background LLM thinking log
# ---------------------------------------------------------------------------


class ListCallbackHandler(BaseCallbackHandler):
    """Collects generated tokens into a list for later display."""

    def __init__(self):
        super().__init__()
        self.logs: list[str] = []

    def on_llm_new_token(self, token: str, **kwargs):
        self.logs.append(token)

    # Tool end (captures the output from the tool)
    def on_tool_end(self, output, **kwargs):
        self.logs.append(f"Action Output: {output}\n")

    # Agent observation hook (LangChain 0.1 compatibility)
    def on_agent_finish(self, output, **kwargs):
        # Some agents send their observation here
        if output:
            self.logs.append(f"Agent Observation: {output}\n")

# ---------------------------------------------------------------------------
# Log formatting helper
# ---------------------------------------------------------------------------

import re


def _format_llm_logs(tokens: list[str]) -> str:
    """
    LLMã®ãƒ­ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ãƒªã‚¹ãƒˆã‚’ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã”ã¨ã«ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã§è¦‹ã‚„ã™ãæ•´å½¢ã—ã¾ã™ã€‚
    **Action Input**ã®æ¬¡ã®è¡Œã‹ã‚‰**Action Output**ã®å‰ã®è¡Œã¾ã§ã¯codeãƒ–ãƒ­ãƒƒã‚¯ã§è¡¨ç¤ºã—ã¾ã™ã€‚
    """
    text = "".join(tokens)

    # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã”ã¨ã«åˆ†å‰²
    # ã¾ãšå„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®è¦‹å‡ºã—ã‚’æŒ¿å…¥
    text = re.sub(r"Thought:\s*", "\n\n**Thought**\n\n", text)
    text = re.sub(r"Action:\s*", "\n\n**Action**\n\n", text)
    text = re.sub(r"Action Input:\s*", "\n\n**Action Input**\n\n", text)
    text = re.sub(r"Action Output:\s*", "\n\n**Action Output**\n\n", text)
    text = re.sub(r"Agent Observation:\s*", "\n\n**Agent Observation**\n\n", text)

    # Action Input ã‹ã‚‰ Action Output ã¾ã§ã€Action Output ã‹ã‚‰æ¬¡ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¾ãŸã¯æ–‡æœ«ã¾ã§ã‚’codeãƒ–ãƒ­ãƒƒã‚¯ã«ã™ã‚‹
    # (?s)ã§æ”¹è¡Œã‚‚å«ã‚ã¦ãƒãƒƒãƒ

    def code_block_replacer_input(match):
        code_content = match.group(1).strip()
        return f"\n\n**Action Input**\n\n```python\n{code_content}\n```\n\n"

    def code_block_replacer_output(match):
        code_content = match.group(1).strip()
        return f"\n\n**Action Output**\n\n```python\n{code_content}\n```\n\n"

    # **Action Input** ã‹ã‚‰ **Action Output** ã¾ã§ã‚’æŠ½å‡ºã—ã€codeãƒ–ãƒ­ãƒƒã‚¯ã§å›²ã‚€
    text = re.sub(
        r"\*\*Action Input\*\*\n(.*?)(?=\n\*\*Action Output\*\*)",
        code_block_replacer_input,
        text,
        flags=re.DOTALL,
    )

    # **Action Output** ã‹ã‚‰æ¬¡ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¾ãŸã¯æ–‡æœ«ã¾ã§ã‚’æŠ½å‡ºã—ã€codeãƒ–ãƒ­ãƒƒã‚¯ã§å›²ã‚€
    text = re.sub(
        r"\*\*Action Output\*\*\n(.*?)(?=\n\*\*|\Z)",
        code_block_replacer_output,
        text,
        flags=re.DOTALL,
    )

    return text.strip()

# ---------------------------------------------------------------------------
# Prepare Task Runner (ä¸¦åˆ—å®Ÿè¡Œç”¨ãƒ˜ãƒ«ãƒ‘ãƒ¼)
# ---------------------------------------------------------------------------

def _run_prepare_task(
    task: dict[str, Any],
    base_df_dict: Dict[str, pd.DataFrame],
    api_key: str,
) -> tuple[str, pd.DataFrame | None, bool, str, list[str]]:
    """
    1 ã¤ã® prepare ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã—ã¦çµæœã‚’è¿”ã™ã€‚

    Returns
    -------
    (output_df_name, df_output_or_None, success, error_message)
    Streamlit API ã¯ä½¿ç”¨ã—ãªã„ãŸã‚ä¸¦åˆ—ã‚¹ãƒ¬ãƒƒãƒ‰ã‹ã‚‰å®‰å…¨ã«å‘¼ã³å‡ºã›ã‚‹ã€‚
    """
    import os

    output_df_name: str = task.get("output")
    handler = ListCallbackHandler()
    try:
        # --- å…¥åŠ› DataFrame æº–å‚™ ---
        input_df_dict: Dict[str, pd.DataFrame] = {
            name: base_df_dict[name].copy() for name in task.get("input", []) if name in base_df_dict
        }

        if not input_df_dict:
            return output_df_name, None, False, "å…¥åŠ› DataFrame ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"

        # --- LLM Agent å®Ÿè¡Œ ---
        prepare_agent = create_pandas_dataframe_agent(
            llm=ChatOpenAI(model="gpt-4.1", temperature=0, api_key=api_key),
            df=input_df_dict,
            agent_type="zero-shot-react-description",
            verbose=True,
            allow_dangerous_code=True,
            return_intermediate_steps=True,
            include_df_in_result=True,
            df_exec_instruction=True,
            agent_executor_kwargs={"handle_parsing_errors": True},
        )

        prompt_for_data = f"""\
ã‚ãªãŸã¯å„ªç§€ãªãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆã§ã™ã€‚
# task
{task['task']}
# input
{task['input']}
# output
{output_df_name}
# output_columns
{task['output_columns']}

# æ³¨æ„ç‚¹
- ã¾ãšåˆã‚ã«inputã®å„ãƒ‡ãƒ¼ã‚¿ã«ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã‹head()ã§ç¢ºèªã—ã¦ãã ã•ã„ã€‚ã‚¢ã‚¯ã‚»ã‚¹ä¸å¯ã®å ´åˆã®ãã®æ—¨å›ç­”ã—ã¦å‡¦ç†ã‚’çµ‚äº†ã—ã¦ãã ã•ã„ã€‚
- å¤‰æ•°ã€ä¸­é–“ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã¯å°ã¾ã‚ã«head()ã‚’å®Ÿè¡Œã—ã¦æƒ³å®šé€šã‚Šä½œæˆã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚
- outputã‚’ç”Ÿæˆã—ãŸã‚‰ã€PythonAstREPLToolã§ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã¦{output_df_name}.jsonã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
{output_df_name}.to_json(
    f"tmp/{output_df_name}.json",
    orient="records",
    date_format="iso",
    date_unit="s",
    index=False,
    force_ascii=False,
)
- æœ€çµ‚ã‚¹ãƒ†ãƒƒãƒ—ã«ã¦ã€{output_df_name}.jsonã®å®Ÿè¡Œã‚¹ãƒ†ãƒƒãƒ—ãŒæ­£å¸¸ã«çµ‚äº†ã—ãŸã“ã¨ã‚’ç¢ºèªã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚
"""

        # Streamlit Callback ã¯ä½¿ç”¨ã—ãªã„
        prepare_agent.invoke({"input": prompt_for_data}, {"callbacks": [handler]})

        # --- ç”Ÿæˆ json èª­ã¿è¾¼ã¿ ---
        json_path = f"tmp/{output_df_name}.json"
        if os.path.exists(json_path):
            df_output = pd.read_json(json_path, orient="records")
            return output_df_name, df_output, True, "", handler.logs

        return output_df_name, None, False, "JSON ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ", handler.logs

    except Exception as exc:
        return output_df_name, None, False, str(exc), []

###############################################################################
# LLM é–¢é€£                                                                    
###############################################################################

def get_llm_client() -> LLMClient | None:
    api_key = st.session_state.get("api_key") 

    if not api_key:
        return None

    if (
        "llm_client" not in st.session_state
        or st.session_state.llm_client is None
        or st.session_state.llm_client.api_key != api_key
    ):
        st.session_state.llm_client = LLMClient(api_key)

    return st.session_state.llm_client

###############################################################################
# Prompt Refinement ãƒãƒ¼ãƒ‰                                                      
###############################################################################

def refine_prompt_node(state: Dict[str, Any]):
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›(ä¾é ¼å†…å®¹)ã¨å¯¾è©±å±¥æ­´ã‚’å…ƒã«LLMã§è©³ç´°åŒ–ã—ã€
    å¿…è¦ã«å¿œã˜ã¦è¿½åŠ ã®è³ªå•ã‚’ç”Ÿæˆã™ã‚‹ã‹ã€åˆ†ææ–¹é‡ã‚’ç¢ºå®šã™ã‚‹ã€‚
    """
    user_prompt: str = state["user_prompt"]
    clarification_history: list[Dict[str, str]] = state.get("clarification_history", [])

    llm_client = get_llm_client()
    if llm_client is None:
        state["refined_prompt"] = user_prompt
        state["need_clarification"] = False
        state["questions"] = None
        return state

    history_str = "\n".join(
        [f"Assistant: {msg['content']}" if msg['role'] == 'assistant' 
         else f"User: {msg['content']}" for msg in clarification_history]
    )

    input_df_list = [{"input_name": n, "input_df": df} for n, df in st.session_state.initial_df_dict.items()]
    df_info = get_dataframe_info(input_df_list)

    refine_prompt_text = f"""
ã‚ãªãŸã¯å„ªç§€ãªãƒˆãƒ¬ã‚¸ãƒ£ãƒªãƒ¼ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆã®å°‚é–€å®¶ã§ã™ã€‚
ä»¥ä¸‹ã®ä¾é ¼å†…å®¹ã¨ã“ã‚Œã¾ã§ã®è³ªç–‘å¿œç­”ã‚’è¸ã¾ãˆã€åˆ†ææ–¹é‡ã‚’ç«‹ã¦ã¦ãã ã•ã„ã€‚

# ä¾é ¼å†…å®¹
{user_prompt}

# ã‚¤ãƒ³ãƒ—ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã®æ¦‚è¦
{df_info}

# ã“ã‚Œã¾ã§ã®è³ªç–‘å¿œç­”
{history_str if history_str else "ãªã—"}

# ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯
1. ä¾é ¼å†…å®¹ã¨è³ªç–‘å¿œç­”ã‚’ç†è§£ã—ã€ã‚¤ãƒ³ãƒ—ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ãŸåˆ†ææ–¹é‡ã‚’æ˜ç¢ºã«ã—ã¾ã™ã€‚
2. ä¸æ˜ãªç‚¹ãŒã‚ã‚Œã°å…·ä½“çš„ãªè³ªå•ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚ã‚ã‚‹ç¨‹åº¦æ¨æ¸¬ã§ãã‚‹å†…å®¹ã¯è³ªå•ã—ãªã„ã‹ã€ã‚¯ãƒ­ãƒ¼ã‚ºãƒ‰ã‚¯ã‚¨ã‚¹ãƒãƒ§ãƒ³ã«ã—ã¦ãã ã•ã„ã€‚
3. è³ªå•ã¯ä¸å¯§ã§ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãªå£èª¿ã§è¡Œã£ã¦ãã ã•ã„ã€‚
4. ã™ã¹ã¦ã®æƒ…å ±ãŒæƒã£ã¦ã„ã‚‹å ´åˆã¯ã€èƒŒæ™¯ã‚„æ„å›³ã‚’è¸ã¾ãˆãŸè©³ç´°ãªåˆ†ææ–¹é‡ã‚’ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§ä½œæˆã—ã¦ãã ã•ã„ã€‚

# é‡è¦ãƒ«ãƒ¼ãƒ«
- **`# ã“ã‚Œã¾ã§ã®è³ªç–‘å¿œç­”` ã§è§£æ±ºæ¸ˆã¿ã®å†…å®¹ã¯ã€å†åº¦åŒã˜å†…å®¹ã®è³ªå•ã‚’ã—ãªã„ã§ãã ã•ã„ã€‚**
- ã‚ãªãŸã®æœ€çµ‚ç›®çš„ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®å¯¾è©±ã‚’é€šã˜ã¦ã‚ã„ã¾ã„ã•ã‚’ãªãã—ã€ã‚¤ãƒ³ãƒ—ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ãŸå®Ÿè¡Œå¯èƒ½ãªåˆ†ææ–¹é‡ï¼ˆ`refined_prompt`ï¼‰ã‚’å®Œæˆã•ã›ã‚‹ã“ã¨ã§ã™ã€‚
- ä¸€åº¦ã«å…¨ã¦è³ªå•ã¯ã›ãšã€1,2å€‹ç¨‹åº¦ãšã¤è³ªå•ã—ã¦ãã ã•ã„ã€‚

# è€ƒæ…®äº‹é …
- ã‚ãªãŸã¯å£åº§æ®‹é«˜ã€å–å¼•å±¥æ­´ã€ç‚ºæ›¿ãƒ¬ãƒ¼ãƒˆã€è²·æ›é‡‘ã€å£²æ›é‡‘ã€å€Ÿå…¥é‡‘ã€æŠ•è³‡ã€ãƒ‡ãƒªãƒãƒ†ã‚£ãƒ–ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒã—ã¦ã„ã¾ã™ã€‚
- ã“ã‚Œã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’åŠ å·¥ã—ã¦ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–ã‚„åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆãŒã§ãã¾ã™ã€‚
- ä¾é ¼å†…å®¹ã‚’ãŸã å¯¾å¿œã™ã‚‹ã ã‘ã§ãªãã€èƒŒæ™¯ã®ãƒ‹ãƒ¼ã‚ºã«ç­”ãˆã‚‰ã‚Œã‚‹ã‚ˆã†ã«å›ç­”æ–¹é‡ã‚’ç«‹ã¦ã¦ãã ã•ã„ã€‚

# å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
å¿…ãšä»¥ä¸‹ã®JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚èª¬æ˜ã¯ä¸è¦ã§ã™ã€‚
- ä¾é ¼å†…å®¹ã‚’åˆ†æã™ã‚‹ã®ã«æƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆ:
  `{{"need_clarification": true, "questions": ["å…·ä½“çš„ãªè³ªå•1", "å…·ä½“çš„ãªè³ªå•2", ...]}}`
- åˆ†ææ–¹é‡ã‚’ç«‹ã¦ã‚‹ã®ã«ååˆ†ãªæƒ…å ±ãŒã‚ã‚‹å ´åˆ:
  `{{"need_clarification": false, "refined_prompt": "## åˆ†ææ–¹é‡\\n..."}}`
"""
    refine_llm = ChatOpenAI(
        model="gpt-4.1",
        api_key=llm_client.api_key,
        verbose=True,
    ).with_structured_output(RefinePromptFormat)
    
    response = refine_llm.invoke(refine_prompt_text)
    
    state["need_clarification"] = response.need_clarification
    state["questions"] = response.questions
    state["refined_prompt"] = response.refined_prompt
    
    # ã“ã®ãƒãƒ¼ãƒ‰ã¯Streamlitã®UIè¦ç´ ã‚’ç›´æ¥æ“ä½œã—ãªã„
    return state


###############################################################################
# Plan ç”Ÿæˆãƒãƒ¼ãƒ‰                                                              
###############################################################################

def generate_plan_node(state: Dict[str, Any]):
    # st.markdown('<h3 class="section-header">ã‚¿ã‚¹ã‚¯ä¸€è¦§</h3>', unsafe_allow_html=True)
    # refine ãƒãƒ¼ãƒ‰ã§è©³ç´°åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒã‚ã‚Œã°ãã¡ã‚‰ã‚’å„ªå…ˆ
    if "refined_prompt" in st.session_state:
        user_prompt: str = st.session_state.refined_prompt
    else:
        st.warning("è©³ç´°åŒ–ã•ã‚ŒãŸä¾é ¼å†…å®¹ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        user_prompt: str = st.session_state.user_prompt
    df_overview: str = state["df_overview"]

    plan_prompt_text = f"""
ã‚ãªãŸã¯çµŒå–¶è€…ã®æ„æ€æ±ºå®šã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹å„ªç§€ãªãƒ‡ãƒ¼ã‚¿ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚           
ä»¥ä¸‹ã®ä¾é ¼å†…å®¹ã®æ„å›³ã‚’ã‚ˆãè€ƒãˆã€ä¾é ¼å†…å®¹ã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã®å…·ä½“çš„ãªã‚¹ãƒ†ãƒƒãƒ—ã«ç´°åˆ†åŒ–ã—ã¦ãã ã•ã„ã€‚
# ä¾é ¼å†…å®¹
{user_prompt}

# ãƒ‡ãƒ¼ã‚¿ã®æ¦‚è¦
{df_overview}

# æ‰‹é †
1. ä¾é ¼å†…å®¹ã‚’ç†è§£ã™ã‚‹ã€‚èƒŒæ™¯ã«ã‚ã‚‹ãƒ‹ãƒ¼ã‚ºã‚‚å«ã‚ã¦ç†è§£ã™ã‚‹ã€‚
2. ãƒ‡ãƒ¼ã‚¿ã®æ¦‚è¦ã‚’ç¢ºèªã™ã‚‹
3. ä¾é ¼å†…å®¹ã«å›ç­”ã™ã‚‹ãŸã‚ã«ã©ã®ã‚ˆã†ãªåˆ†æã‚„ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–ã‚’è¡Œã†ã‹æ¤œè¨ã™ã‚‹ã€‚ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–ã¯å¿…è¦æœ€å°é™ã®ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãŒæœ›ã¾ã—ãã€ãƒ•ã‚£ãƒ«ã‚¿ã‚„ã—ãã„å€¤è¨­å®šãŒã§ãã‚‹ã‚ˆã†ã«ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªæ“ä½œãŒã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚
4. ä¾é ¼å†…å®¹ã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã«prepare(è¤‡æ•°å¯)ã€visualize(è¤‡æ•°å¯)ã€report(1ã¤ã®ã¿)ã®3ã‚¹ãƒ†ãƒƒãƒ—ã®ã‚¿ã‚¹ã‚¯ã«ç´°åˆ†åŒ–ã™ã‚‹
5. ç´°åˆ†åŒ–ã•ã‚ŒãŸå„ã‚¿ã‚¹ã‚¯ã«ã¤ã„ã¦ä»¥ä¸‹ã®å†…å®¹ã‚’å›ç­”ã™ã‚‹
    - category(prepare/visualize/report)
    - task(ä¸ãˆã‚‰ã‚ŒãŸdfã‚’ã‚¤ãƒ³ãƒ—ãƒƒãƒˆã«ã©ã®ã‚ˆã†ãªåŠ å·¥ã¾ãŸã¯åˆ†æã‚’ã™ã‚‹ã‹å…·ä½“çš„ã‹ã¤è©³ç´°ã«æ˜è¨˜)
    - input(è¤‡æ•°ã‚ã‚‹å ´åˆã¯ãƒªã‚¹ãƒˆ)
    - output(df_ã‹ã‚‰å§‹ã¾ã‚‹å˜ä¸€ã®dataframeåâ€»visualizeã¨reportã®å ´åˆã¯ã‚¹ãƒšãƒ¼ã‚¹)
    - output_columns(å‡ºåŠ›ã™ã‚‹ã‚«ãƒ©ãƒ ã®ãƒªã‚¹ãƒˆâ€»å¯èƒ½ãªé™ã‚Šinputã®ã‚«ãƒ©ãƒ ã‚’æ®‹ã™â€»å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ãŒã‚ã‚‹å ´åˆã®ã¿)
6. prepareã®ã‚¿ã‚¹ã‚¯ã¯ã€visualizeã¨reportã®ã‚¿ã‚¹ã‚¯ã«å¿…è¦ãªå„dataframeã”ã¨ã«ç´°åˆ†åŒ–ã—ã¦ãã ã•ã„ã€‚(1ã‚¿ã‚¹ã‚¯1ã¤ã®dataframeã‚’outputã¨ã—ã¦æŒã¤)
7. prepareã®ã‚¿ã‚¹ã‚¯ã¯ã€å„dataframeã‚’ã‚¤ãƒ³ãƒ—ãƒƒãƒˆã«pythonã§å®Ÿæ–½ã§ãã‚‹åˆ†æå†…å®¹ã«ã—ã¦ãã ã•ã„ã€‚
    """

    plan_prompt = PromptTemplate.from_template("{input}")
    plan_llm = ChatOpenAI(
        model="gpt-4.1",
        temperature=0,
        api_key=get_llm_client().api_key,
        verbose=True,
    ).with_structured_output(ResponseFormatter)

    plan_chain = plan_prompt | plan_llm
    response_plan = plan_chain.invoke({"input": plan_prompt_text})
    if isinstance(response_plan, ResponseFormatter):
        plans = response_plan.plans
    else:
        plans = response_plan

    plan_df = pd.DataFrame([p.model_dump() if isinstance(p, Plan) else p for p in plans])
    plan_df["status"] = "â¬œ"
    plan_df = plan_df.reindex(
        columns=["category", "status", "task", "input", "output", "output_columns"]
    )

    st.session_state.plan = plan_df
    # state æ›´æ–°
    state["plan_df"] = plan_df
    return state

###############################################################################
# Prepare ãƒãƒ¼ãƒ‰                                                               
###############################################################################

def prepare_node(state: Dict[str, Any]):
    import os

    plan_df: pd.DataFrame = state["plan_df"]
    prepare_tasks = plan_df[plan_df["category"] == "prepare"].to_dict(orient="records")

    st.markdown('<h3 class="section-header">ãƒ‡ãƒ¼ã‚¿</h3>', unsafe_allow_html=True)
    with st.spinner("ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆä¸­"):
        with st.expander("ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿", expanded=True):
            # --- ã‚¿ãƒ– & ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ç”Ÿæˆ ---
            output_names = [task["output"] for task in prepare_tasks]
            tabs = st.tabs(output_names)
            tab_placeholders: Dict[str, Any] = {}
            for idx, name in enumerate(output_names):
                with tabs[idx]:
                    tab_placeholders[name] = st.empty()

            work_df_dict: Dict[str, pd.DataFrame] = {}
            pending_tasks = prepare_tasks.copy()
            api_key = get_llm_client().api_key

            while pending_tasks:
                # ä¾å­˜ãŒè§£æ±ºæ¸ˆã¿ã®ã‚¿ã‚¹ã‚¯ã‚’æŠ½å‡º
                ready_tasks = [t for t in pending_tasks if all(
                    n in work_df_dict or n in st.session_state.initial_df_dict for n in t["input"]
                )]

                if not ready_tasks:
                    st.error("ä¾å­˜é–¢ä¿‚ã‚’è§£æ±ºã§ããªã„ prepare ã‚¿ã‚¹ã‚¯ãŒã‚ã‚Šã¾ã™ã€‚å¾ªç’°å‚ç…§ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
                    break

                # "ğŸ”„" ã«ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°
                for t in ready_tasks:
                    st.session_state.plan.loc[
                        st.session_state.plan["task"] == t["task"], "status"
                    ] = "ğŸ”„"
                st.session_state.plan_placeholder.dataframe(st.session_state.plan, use_container_width=True)

                # ä¸¦åˆ—å®Ÿè¡Œç”¨ã«ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’æ¸¡ã™
                base_df_dict_snapshot = {**st.session_state.initial_df_dict, **work_df_dict}

                with ThreadPoolExecutor(max_workers=4) as executor:
                    future_to_task = {
                        executor.submit(_run_prepare_task, t, base_df_dict_snapshot, api_key): t for t in ready_tasks
                    }

                    for future in as_completed(future_to_task):
                        task = future_to_task[future]
                        output_name, df_output, success, err_msg, logs = future.result()

                        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°
                        st.session_state.plan.loc[
                            st.session_state.plan["task"] == task["task"], "status"
                        ] = "âœ…" if success else "âŒ"

                        placeholder = tab_placeholders.get(output_name)

                        if success and df_output is not None:
                            work_df_dict[output_name] = df_output
                            if placeholder is not None:
                                with placeholder.container():
                                    st.info(f"ã‚¿ã‚¹ã‚¯: {task['task']}")
                                    st.dataframe(df_output[:100], use_container_width=True)
                                    if logs:
                                        with st.expander("LLM Log"):
                                            st.markdown(_format_llm_logs(logs), unsafe_allow_html=True)
                        else: # if success false show error and logs
                            # å¤±æ•—ã—ãŸå ´åˆã§ã‚‚ã‚¿ãƒ–ã‚’æ®‹ã™ãŸã‚ã€ãƒ€ãƒŸãƒ¼ã®ç©º DataFrame ã‚’ç™»éŒ²
                            work_df_dict[output_name] = pd.DataFrame()
                            if placeholder is not None:
                                with placeholder.container():
                                    st.error(f"{task['task']} å¤±æ•—: {err_msg}")
                                    if logs:
                                        with st.expander("LLM Log"):
                                            st.markdown(_format_llm_logs(logs), unsafe_allow_html=True)

                        # ãƒ—ãƒ©ãƒ³è¡¨ã‚’å†æç”»
                        st.session_state.plan_placeholder.dataframe(st.session_state.plan, use_container_width=True)

                # å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯ã‚’ pending ã‹ã‚‰é™¤å»
                pending_tasks = [t for t in pending_tasks if t not in ready_tasks]

        st.session_state.plan_placeholder.dataframe(st.session_state.plan, use_container_width=True)

        st.session_state.work_df_dict = work_df_dict
        state["work_df_dict"] = work_df_dict
    return state

###############################################################################
# Visualize Task Runner (ä¸¦åˆ—å®Ÿè¡Œç”¨ãƒ˜ãƒ«ãƒ‘ãƒ¼)
###############################################################################

def _run_visualize_task(
    task: dict[str, Any],
    base_df_dict: Dict[str, pd.DataFrame],
    api_key: str,
    safety_checker: SafetyChecker,
) -> tuple[str, str | None, bool, str]:
    """Heavy part of visualize task (LLM code generation & safety check).

    Returns
    -------
    (task_id, code_to_run_or_None, safe_ok, error_msg)
    """
    try:
        # DataFrame æƒ…å ±ã‚’æ–‡å­—åˆ—åŒ–
        input_df_names = task["input"]
        input_df_list = [
            {"input_name": n, "input_df": base_df_dict.get(n)} for n in input_df_names if n in base_df_dict
        ]

        df_info = get_dataframe_info(input_df_list)

        from llm_client import LLMClient
        llm_client = LLMClient(api_key)
        generated_code = llm_client.generate_code(task, df_info)

        # DataFrame å‚ç…§æ›¸ãæ›ãˆ
        replaced_code = replace_df_references(generated_code, input_df_names, base_df_dict)

        # å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯
        is_safe, _ = safety_checker.is_safe(replaced_code)
        code_to_use = replaced_code

        if not is_safe:
            fixed_code = llm_client.fix_code(replaced_code, "å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯ã«å¤±æ•—ã—ã¾ã—ãŸ", task, df_info)
            fixed_code = replace_df_references(fixed_code, input_df_names, base_df_dict)
            is_safe, _ = safety_checker.is_safe(fixed_code)
            code_to_use = fixed_code if is_safe else None

        if code_to_use is None:
            return task["task"], None, False, "ã‚³ãƒ¼ãƒ‰ã®å®‰å…¨æ€§ã‚’ç¢ºä¿ã§ãã¾ã›ã‚“ã§ã—ãŸ"

        return task["task"], code_to_use, True, ""

    except Exception as e:
        return task["task"], None, False, str(e)

###############################################################################
# Visualize ãƒãƒ¼ãƒ‰                                                             
###############################################################################

def visualize_node(state: Dict[str, Any]):
    import os

    # work_df_dict ãŒå­˜åœ¨ã—ãªã„ã‚±ãƒ¼ã‚¹ã«å‚™ãˆã¦åˆæœŸåŒ–
    if "work_df_dict" not in st.session_state:
        st.session_state.work_df_dict = {}

    plan_df: pd.DataFrame = state["plan_df"]
    visualize_tasks = plan_df[plan_df["category"] == "visualize"].to_dict(orient="records")

    def fix_code(code: str, error_message: str, task: dict[str, Any], df_info: str):
        """ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰ã§å‘¼ã³å‡ºã™ã‚³ãƒ¼ãƒ‰ä¿®æ­£ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£"""
        input_df_names = task["input"]

        try:
            fixed_code = st.session_state.llm_client.fix_code(code, error_message, task, df_info)
            is_safe_fixed, _ = st.session_state.safety_checker.is_safe(fixed_code)
            if not is_safe_fixed:
                return None
            return replace_df_references(fixed_code, input_df_names, st.session_state.initial_df_dict)
        except Exception as exc:
            logger.exception("fix_code å¤±æ•—", exc_info=exc)
            return None

    st.markdown('<h3 class="section-header">ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«</h3>', unsafe_allow_html=True)
    with st.spinner("ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ã‚’ç”Ÿæˆä¸­"):
        with st.expander("ç”Ÿæˆã•ã‚ŒãŸãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«", expanded=True):
            vis_tabs = st.tabs([f"visual_{i+1}" for i in range(len(visualize_tasks))])

            # ã‚¿ãƒ–ã”ã¨ã«ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ç”¨æ„
            tab_placeholders: Dict[str, Any] = {}
            for idx, task in enumerate(visualize_tasks):
                with vis_tabs[idx]:
                    tab_placeholders[task["task"]] = st.container()

            # äº‹å‰ã«ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ ğŸ”„ ã«
            for t in visualize_tasks:
                st.session_state.plan.loc[
                    st.session_state.plan["task"] == t["task"], "status"
                ] = "ğŸ”„"
            st.session_state.plan_placeholder.dataframe(st.session_state.plan, use_container_width=True)

            # --- å…¥åŠ› DataFrame ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆä½œæˆ ---
            base_df_dict_snapshot = {**st.session_state.initial_df_dict, **st.session_state.get("work_df_dict", {})}
            api_key = get_llm_client().api_key
            safety_checker = st.session_state.safety_checker

            # --- ä¾å­˜ DF ãŒå­˜åœ¨ã—ãªã„ã‚¿ã‚¹ã‚¯ã‚’é™¤å¤–ã—ã€ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’âŒã«è¨­å®š ---
            runnable_tasks = []
            for t in visualize_tasks:
                missing_inputs = [n for n in t["input"] if n not in base_df_dict_snapshot]
                if missing_inputs:
                    tab_container = tab_placeholders[t["task"]]
                    with tab_container:
                        st.warning(f"å…¥åŠ› DataFrame ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {', '.join(missing_inputs)}")
                    st.session_state.plan.loc[
                        st.session_state.plan["task"] == t["task"], "status"
                    ] = "âŒ"
                else:
                    runnable_tasks.append(t)

            st.session_state.plan_placeholder.dataframe(st.session_state.plan, use_container_width=True)

            if not runnable_tasks:
                state["generated_codes"] = st.session_state.generated_codes
                return state

            with ThreadPoolExecutor(max_workers=4) as executor:
                future_to_task = {
                    executor.submit(_run_visualize_task, t, base_df_dict_snapshot, api_key, safety_checker): t for t in runnable_tasks
                }

                for future in as_completed(future_to_task):
                    task = future_to_task[future]
                    tab_container = tab_placeholders[task["task"]]

                    with tab_container:
                        st.info(f"ã‚¿ã‚¹ã‚¯: {task['task']}")
                        with st.spinner(f"ç”Ÿæˆä¸­"):
                            try:
                                task_id, code_to_run, safe_ok, err_msg = future.result()
                            except Exception as e:
                                code_to_run, safe_ok, err_msg = None, False, str(e)

                            if not safe_ok or code_to_run is None:
                                st.error(f"ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ç”Ÿæˆå¤±æ•—: {err_msg}")
                                st.session_state.plan.loc[
                                    st.session_state.plan["task"] == task["task"], "status"
                                ] = "âŒ"
                                st.session_state.plan_placeholder.dataframe(st.session_state.plan, use_container_width=True)
                                continue

                        # ---------------- ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œ ----------------
                        output_placeholder = tab_container.empty()

                        def run_and_render(code: str, suffix: str = "") -> tuple[bool, str | None, str | None]:
                            output_placeholder.empty()
                            with output_placeholder.container():
                                success_inner, stdout_inner, err_inner = (
                                    st.session_state.code_executor.execute_code(code)
                                )

                                if stdout_inner:
                                    with st.expander(f"log{suffix}"):
                                        st.text(stdout_inner)
                                with st.expander(f"code{suffix}"):
                                    st.code(code, language="python")
                            return success_inner, stdout_inner, err_inner

                        # 1 å›ç›®
                        success, stdout, err = run_and_render(code_to_run)

                    if not success or err:
                        with tab_container.empty() as tab_container:
                            # ä¿®æ­£è©¦è¡Œ
                            input_df_list = [
                                {"input_name": n, "input_df": base_df_dict_snapshot.get(n)} for n in task["input"] if n in base_df_dict_snapshot
                            ]
                            df_info = get_dataframe_info(input_df_list)
                            fixed_code = fix_code(code_to_run, err, task, df_info)
                            if fixed_code:
                                st.warning(f"ä¿®æ­£å¾Œã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œ")
                                success, stdout, err = run_and_render(fixed_code, "(ä¿®æ­£å¾Œ)")
                                if success:
                                    code_to_run = fixed_code
                                else:
                                    st.error(err)

                    # ç”Ÿæˆã‚³ãƒ¼ãƒ‰ä¿å­˜
                    st.session_state.generated_codes.append({"task": task["task"], "code": code_to_run})

                    # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°
                    st.session_state.plan.loc[
                        st.session_state.plan["task"] == task["task"], "status"
                    ] = "âœ…" if success else "âŒ"

                    # ãƒ—ãƒ©ãƒ³è¡¨å†æç”»
                    st.session_state.plan_placeholder.dataframe(st.session_state.plan, use_container_width=True)

        state["generated_codes"] = st.session_state.generated_codes
    return state

###############################################################################
# Report ãƒãƒ¼ãƒ‰                                                                
###############################################################################

def report_node(state: Dict[str, Any]):
    st.session_state.generated_report = []
    plan_df: pd.DataFrame = state["plan_df"]
    report_tasks = plan_df[plan_df["category"] == "report"].to_dict(orient="records")

    llm = get_llm_client()
    if not llm:
        st.error("API Key æœªè¨­å®šã®ãŸã‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—")
        return state

    for task in report_tasks:
        # prepare ã‚¿ã‚¹ã‚¯ã®æ¦‚è¦ã‚’æ–‡å­—åˆ—åŒ–
        prepare_plan = plan_df[plan_df["category"] == "prepare"][["task", "input", "output"]].to_dict(orient="records")

        input_df_names = task["input"]
        # å…¥åŠ› DataFrame ã‚’ dict å½¢å¼ {name: df} ã«å¤‰æ›
        input_df_dict = {name: st.session_state.work_df_dict[name] for name in input_df_names if name in st.session_state.work_df_dict}
        for name in input_df_names:
            if name in st.session_state.initial_df_dict:
                input_df_dict[name] = st.session_state.initial_df_dict[name]

        prompt_for_report = f"""
ã‚ãªãŸã¯è²¡å‹™åˆ†æã®çµŒé¨“è±Šå¯Œãªãƒ‡ãƒ¼ã‚¿ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®inputã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚‚ã¨ã«ã€taskã«å¾“ã£ã¦åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ä¾é ¼å†…å®¹
{st.session_state.user_prompt}

# ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯
{task['task']}

# ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯ã®èƒŒæ™¯
{st.session_state.refined_prompt}

# ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯ã®input
{input_df_names}

# ã“ã“ã¾ã§ã®ãƒ‡ãƒ¼ã‚¿æº–å‚™ã®çµŒç·¯
{prepare_plan}

# ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯
inputã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚ˆãå‚ç…§ã—ã€taskã®èƒŒæ™¯ã‚’è¸ã¾ãˆãŸä¸Šã§å…·ä½“çš„ãªåˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

# æ³¨æ„ç‚¹
- ã¾ãšåˆã‚ã«inputã®ãƒ‡ãƒ¼ã‚¿ã«ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã‹head()ãªã©ã§ç¢ºèªã—ã¦ãã ã•ã„ã€‚ã‚¢ã‚¯ã‚»ã‚¹ä¸å¯ã®å ´åˆã®ãã®æ—¨å›ç­”ã—ã¦å‡¦ç†ã‚’çµ‚äº†ã—ã¦ãã ã•ã„ã€‚
- æ¨æ¸¬ã§å›ç­”ã›ãšã€PythonAstREPLToolã‚’ä½¿ç”¨ã—ã¦inputã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦åˆ†æã‚’è¡Œã„ã€ç¢ºèªã—ã¦ç¤ºå”†ã«å¯Œã‚“ã è¦³ç‚¹ã‚’ç¤ºã—ã¦ãã ã•ã„ã€‚
- å…·ä½“çš„ãªæ•°å€¤ãªã©å®šé‡çš„ãªãƒ‡ãƒ¼ã‚¿ã‚‚æ ¹æ‹ ã«çµè«–ã‚’ç¤ºã—ã¦ãã ã•ã„ã€‚
- ãƒ¬ãƒãƒ¼ãƒˆã¯markdownå½¢å¼ã§ä½œæˆã—ã¦ãã ã•ã„ã€‚
- ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯å¿…ãš `<div data-card>` ã¨ `</div>` ã§å›²ã¿ã€ã‚«ãƒ¼ãƒ‰å½¢å¼ã§è¦–è¦šçš„ã«åŒºåˆ‡ã£ã¦ãã ã•ã„ã€‚
- å¼·èª¿ã¾ãŸã¯è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«ã¯ `<p data-alert="ok|warn|info"> ... </p>` ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
- ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼ã€åˆ†ææ–¹æ³•ã€åˆ†æçµæœã€ç™ºè¦‹äº‹é …ã€åˆ†æã§ä½¿ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚ã¦ãã ã•ã„ã€‚
"""
        st.markdown('<h3 class="section-header">ãƒ¬ãƒãƒ¼ãƒˆ</h3>', unsafe_allow_html=True)
        
        report_agent = create_pandas_dataframe_agent(
            llm=ChatOpenAI(model="gpt-4.1", api_key=llm.api_key),
            df=input_df_dict,
            agent_type="tool-calling",
            verbose=True,
            allow_dangerous_code=True,
            return_intermediate_steps=True,
            df_exec_instruction=False,
            agent_executor_kwargs={"handle_parsing_errors": True},
            )
        with st.spinner(f"ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­[{task['task']}]"):
            handler = ListCallbackHandler()
            res = report_agent.invoke({"input": prompt_for_report}, {"callbacks": [handler]})
            st.session_state.generated_report.append(res["output"])
            #st.markdown(res["output"]) # codeãƒ–ãƒ­ãƒƒã‚¯ã§ã¯ãªãMarkdownã§è¡¨ç¤º
            with st.expander("ãƒ¬ãƒãƒ¼ãƒˆ"):
                # res["output"]ã®ä¸­ã«ã€</div> <div data-card> #ã®ã‚ˆã†ãªä¸¦ã³ãŒã‚ã‚Œã°æ”¹è¡Œã‚’å…¥ã‚Œã‚‹
                res["output"] = re.sub(r"<div data-card>[^\n]*", "<div data-card>\n", res["output"])
                st.markdown(res["output"], unsafe_allow_html=True)
                with st.expander("LLM Log"):
                    logs = _format_llm_logs(handler.logs)
                    st.markdown(logs, unsafe_allow_html=True)

        st.session_state.plan.loc[
            st.session_state.plan["task"] == task["task"], "status"
        ] = "âœ…"
        st.session_state.plan_placeholder.dataframe(st.session_state.plan, use_container_width=True)

    state["generated_report"] = st.session_state.generated_report
    return state

###############################################################################
# Flow Builder                                                                 
###############################################################################

def build_plan_flow():
    g = StateGraph(dict)
    g.add_node("refine", refine_prompt_node)
    g.add_node("plan", generate_plan_node)
    g.add_edge("refine", "plan")
    g.set_entry_point("refine")
    g.set_finish_point("plan")
    return g.compile()

def build_execution_flow():
    g = StateGraph(dict)
    g.add_node("prepare", prepare_node)
    g.add_node("visualize", visualize_node)
    g.add_node("report", report_node)
    g.add_edge("prepare", "visualize")
    g.add_edge("visualize", "report")
    g.set_entry_point("prepare")
    g.set_finish_point("report")
    return g.compile()

###############################################################################
# Streamlit Main                                                               
###############################################################################

def main(initial_df_dict: Dict[str, pd.DataFrame]):
    st.set_page_config(page_title="Treasury Agent", layout="wide")
    _inject_report_css()
    initialize_session_state(initial_df_dict)

    # user_prompt = st.text_area("ä¾é ¼å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", height=100)
    # run_button = st.button("å®Ÿè¡Œ", type="primary")

    # ------------------------------------------------------------------
    # 1. ä¾é ¼å…¥åŠ›ãƒ•ã‚§ãƒ¼ã‚º
    # ------------------------------------------------------------------
    with st.container():
        # ä»¥å‰ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ã‚ã‚Œã°è¡¨ç¤º
        prompt_value = st.session_state.get("user_prompt", "")
        user_prompt = st.text_area("ä¾é ¼å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", value=prompt_value, height=100, key="user_prompt_input")
        
        # APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
        if not st.session_state.get("api_key"):
            st.warning("ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰OpenAI API Keyã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")

        generate_plan_button = st.button(
            "ã‚¿ã‚¹ã‚¯ç”Ÿæˆ", 
            type="primary",
            disabled=not st.session_state.get("api_key") or st.session_state.app_status not in ["initial", "plan_generated", "completed"]
        )

    if generate_plan_button:
        if not user_prompt.strip():
            st.warning("ä¾é ¼å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            st.stop()
        
        # å®Ÿè¡Œã®ãŸã³ã«çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
        st.session_state.messages = []
        st.session_state.plan = pd.DataFrame()
        st.session_state.work_df_dict = {}
        st.session_state.generated_codes = []
        st.session_state.generated_report = ""
        st.session_state.refined_prompt = ""
        st.session_state.clarification_history = []
        st.session_state.conversation_context = ConversationContext()
        st.session_state.user_prompt = user_prompt
        st.session_state.app_status = "planning" # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’è¨ˆç”»ä¸­ã«

        st.rerun()

    # ------------------------------------------------------------------
    # 2. ä¾é ¼è©³ç´°åŒ–ãƒ•ã‚§ãƒ¼ã‚º (å¯¾è©±ãƒ«ãƒ¼ãƒ—)
    # ------------------------------------------------------------------
    if st.session_state.app_status == "planning":
        st.markdown("---")
        st.markdown("<h4 class='section-header'>ä¾é ¼å†…å®¹ã®ç¢ºèª</h4>", unsafe_allow_html=True)

        if st.session_state.clarification_history:
            with st.expander("å¯¾è©±å±¥æ­´", expanded=False):
                for message in st.session_state.clarification_history:
                    with st.chat_message(message["role"]):
                        st.markdown(message["content"], unsafe_allow_html=True)

        # å¯¾è©±å±¥æ­´ã®è¡¨ç¤º
        if len(st.session_state.clarification_history) == 0:
            pass
        elif st.session_state.clarification_history[-1]["role"] == "assistant":
            with st.chat_message("assistant"):
                st.markdown(st.session_state.clarification_history[-1]["content"], unsafe_allow_html=True)
        else:
            with st.chat_message("assistant"):
                st.markdown(st.session_state.clarification_history[-2]["content"], unsafe_allow_html=True)
            with st.chat_message("user"):
                st.markdown(st.session_state.clarification_history[-1]["content"], unsafe_allow_html=True)

        # LLMã‚’å‘¼ã³å‡ºã™ã¹ãã‹åˆ¤æ–­ (åˆå›ã¾ãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼è¿”ä¿¡å¾Œ)
        with st.spinner("ç¢ºèªä¸­..."):
            should_run_refine = not st.session_state.clarification_history or st.session_state.clarification_history[-1]["role"] == "user"

            if should_run_refine:
                # with st.spinner("ä¾é ¼å†…å®¹ã‚’åˆ†æãƒ»è©³ç´°åŒ–ã—ã¦ã„ã¾ã™..."):
                state = {
                    "user_prompt": st.session_state.user_prompt,
                    "clarification_history": st.session_state.clarification_history
                }
                
                try:
                    result_state = refine_prompt_node(state)
                    
                    if result_state.get("need_clarification"):
                        questions = result_state.get("questions", [])
                        if questions:
                            st.session_state.clarification_history.append({"role": "assistant", "content": "\n".join(f"{q}" for q in questions)})
                        st.rerun()
                    else:
                        st.session_state.refined_prompt = result_state.get("refined_prompt")
                        st.session_state.app_status = "plan_generating" # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¸
                        st.rerun()

                except Exception as e:
                    st.exception(e)
                    logger.exception("refine_prompt_node å¤±æ•—")
                    st.session_state.app_status = "initial"
                    # st.rerun()

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®å›ç­”ã‚’å¾…ã¤
        if st.session_state.clarification_history and st.session_state.clarification_history[-1]["role"] == "assistant":
            if user_reply := st.chat_input("å›ç­”ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"):
                st.session_state.clarification_history.append({"role": "user", "content": user_reply})
                st.rerun()

    # ------------------------------------------------------------------
    # 2.5 ãƒ—ãƒ©ãƒ³ç”Ÿæˆãƒ•ã‚§ãƒ¼ã‚º
    # ------------------------------------------------------------------
    if st.session_state.app_status == "plan_generating":
        with st.spinner("ã‚¿ã‚¹ã‚¯ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™..."):
            try:
                sleep(.1) 
                df_overview = get_dataframe_info(
                    [{"input_name": n, "input_df": df} for n, df in st.session_state.initial_df_dict.items()]
                )
                state = {
                    "user_prompt": st.session_state.user_prompt,
                    "refined_prompt": st.session_state.refined_prompt,
                    "df_overview": df_overview
                }
                # generate_plan_nodeã¯UIã‚’æ“ä½œã™ã‚‹ãŸã‚ã€ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œ
                generate_plan_node(state)
                st.session_state.app_status = "plan_generated"
                st.rerun()
            except Exception as e:
                st.exception(e)
                logger.exception("generate_plan_node å¤±æ•—")
                st.session_state.app_status = "initial"
                # st.rerun()

    # ------------------------------------------------------------------
    # 3. ãƒ—ãƒ©ãƒ³ç¢ºèªãƒ»ç·¨é›†ãƒ•ã‚§ãƒ¼ã‚º
    # ------------------------------------------------------------------
    if st.session_state.app_status == "plan_generated":
        # st.markdown("---")
        st.markdown("<h3 class='section-header'>ã‚¿ã‚¹ã‚¯ä¸€è¦§</h3>", unsafe_allow_html=True)

        st.info("ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ã‚’ç¢ºèªã—ã€å¿…è¦ã«å¿œã˜ã¦ã‚¿ã‚¹ã‚¯ã®è¿½åŠ ãƒ»å‰Šé™¤ãƒ»ç·¨é›†ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚å•é¡ŒãŒãªã‘ã‚Œã°ã€Œã‚¿ã‚¹ã‚¯å®Ÿè¡Œã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
        
        # å¯¾è©±å±¥æ­´ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦è¡¨ç¤º
        if st.session_state.get("clarification_history"):
            with st.expander("ä¾é ¼å†…å®¹ã®ç¢ºèªå±¥æ­´", expanded=False):
                for message in st.session_state.clarification_history:
                    with st.chat_message(message["role"]):
                        st.markdown(message["content"], unsafe_allow_html=True)

        if st.session_state.get("refined_prompt"):
            with st.expander("è©³ç´°åŒ–ã•ã‚ŒãŸä¾é ¼å†…å®¹", expanded=False):
                st.markdown(st.session_state.refined_prompt, unsafe_allow_html=True)
        
        # data_editorã§ãƒ—ãƒ©ãƒ³ã‚’ç·¨é›†å¯èƒ½ã«ã™ã‚‹
        edited_plan_df = st.data_editor(
            st.session_state.plan,
            use_container_width=True,
            num_rows="dynamic",
            column_config={
                "category": st.column_config.SelectboxColumn(
                    "Category",
                    options=["prepare", "visualize", "report"],
                    required=True,
                ),
                "status": st.column_config.TextColumn("Status", disabled=True),
                "input": st.column_config.ListColumn("Input"),
                "output_columns": st.column_config.ListColumn("Output Columns"),
            },
            key="plan_editor"
        )
        
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ", type="primary"):
                st.session_state.plan = edited_plan_df # å®Ÿè¡Œå‰ã«æœ€æ–°ã®ç·¨é›†å†…å®¹ã‚’ä¿å­˜
                st.session_state.app_status = "executing"
                st.rerun()
        with col2:
            if st.button("æœ€åˆã‹ã‚‰ã‚„ã‚Šç›´ã™"):
                st.session_state.app_status = "initial"
                st.rerun()

    # ------------------------------------------------------------------
    # 4. å®Ÿè¡Œãƒ•ã‚§ãƒ¼ã‚º & 5. çµæœè¡¨ç¤ºãƒ»ãƒãƒ£ãƒƒãƒˆãƒ•ã‚§ãƒ¼ã‚º
    # ------------------------------------------------------------------
    if st.session_state.app_status in ["executing", "completed"]:
        st.markdown("---")
        st.markdown("### ã‚¿ã‚¹ã‚¯å®Ÿè¡ŒçŠ¶æ³")
        st.session_state.plan_placeholder = st.empty()
        st.session_state.plan_placeholder.dataframe(st.session_state.plan, use_container_width=True)

    if st.session_state.app_status == "executing":
        try:
            # å®Ÿè¡Œãƒ•ãƒ­ãƒ¼ã‚’é–‹å§‹
            execution_flow = build_execution_flow()
            state = {
                "plan_df": st.session_state.plan,
                "user_prompt": st.session_state.user_prompt,
                "refined_prompt": st.session_state.get("refined_prompt", ""),
            }
            # å„ãƒãƒ¼ãƒ‰ãŒ session_state.initial_df_dict ã‚’ç›´æ¥å‚ç…§ã™ã‚‹ãŸã‚ã€state ã«å«ã‚ã‚‹å¿…è¦ã¯å¿…ãšã—ã‚‚ãªã„
            execution_flow.invoke(state)

            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ›´æ–°
            context = st.session_state.conversation_context
            context.user_prompt = st.session_state.user_prompt
            context.refined_prompt = st.session_state.get("refined_prompt")
            context.plan = st.session_state.get("plan")
            context.prepare_results = st.session_state.get("work_df_dict")
            context.visualize_results = st.session_state.get("generated_codes")
            context.report = "\n".join(st.session_state.get("generated_report", []))
            # å¯¾è©±å±¥æ­´ã‚‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«è¿½åŠ 
            context.chat_history.extend(st.session_state.clarification_history)

            st.session_state.app_status = "completed"
            st.success("åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
            st.rerun()
            
        except Exception as e:
            st.exception(e)
            logger.exception("execution_flow.invoke å¤±æ•—")
            st.session_state.app_status = "plan_generated" # å¤±æ•—ã—ãŸã‚‰ãƒ—ãƒ©ãƒ³ç·¨é›†ç”»é¢ã«æˆ»ã™
            # st.rerun()

    if st.session_state.app_status == "completed":
        # ç”Ÿæˆãƒ‡ãƒ¼ã‚¿
        if st.session_state.get("work_df_dict"):
            st.markdown('<h3 class="section-header">ãƒ‡ãƒ¼ã‚¿</h3>', unsafe_allow_html=True)
            with st.expander("ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿", expanded=False):
                tabs = st.tabs(list(st.session_state.work_df_dict.keys()))
                for idx, (df_name, df_val) in enumerate(st.session_state.work_df_dict.items()):
                    with tabs[idx]:
                        task_info = st.session_state.plan[st.session_state.plan["output"] == df_name]
                        if not task_info.empty:
                            st.info(f"ã‚¿ã‚¹ã‚¯: {task_info.iloc[0]['task']}")
                        st.dataframe(df_val[:100], use_container_width=True)

        # ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«å†æç”»
        if st.session_state.get("generated_codes"):
            st.markdown('<h3 class="section-header">ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«</h3>', unsafe_allow_html=True)
            with st.expander("ç”Ÿæˆã•ã‚ŒãŸãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«", expanded=False):
                vis_tabs = st.tabs([f"visual_{i+1}" for i in range(len(st.session_state.generated_codes))])
                for idx, gen_code_info in enumerate(st.session_state.generated_codes):
                    with vis_tabs[idx]:
                        try:
                            task_description = gen_code_info.get("task", "ã‚¿ã‚¹ã‚¯ã®èª¬æ˜ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                            gen_code = gen_code_info.get("code", "")
                            st.info(f"ã‚¿ã‚¹ã‚¯: {task_description}")
                            success, stdout, err = st.session_state.code_executor.execute_code(gen_code)
                            if not success and err: st.error(err)
                            if stdout:
                                with st.expander("log"): st.text(stdout)
                            with st.expander("code"): st.code(gen_code, language="python")
                        except Exception as e:
                            st.error(f"å†æç”»å¤±æ•—: {e}")

        # ãƒ¬ãƒãƒ¼ãƒˆ
        if st.session_state.get("generated_report"):
            st.markdown('<h3 class="section-header">ãƒ¬ãƒãƒ¼ãƒˆ</h3>', unsafe_allow_html=True)
            for report in st.session_state.generated_report:
                with st.expander("ãƒ¬ãƒãƒ¼ãƒˆ", expanded=True):
                    report = re.sub(r"<div data-card>[^\n]*", "<div data-card>\n", report)
                    st.markdown(report, unsafe_allow_html=True)

    # ------------------------------------------------------------------
    # 5. ãƒãƒ£ãƒƒãƒˆãƒ•ã‚§ãƒ¼ã‚º
    # ------------------------------------------------------------------
    is_report_ready = st.session_state.app_status == "completed" and bool(st.session_state.conversation_context.report)

    if is_report_ready:
        st.divider()
        st.markdown('<h3 class="section-header">ãƒãƒ£ãƒƒãƒˆ</h3>', unsafe_allow_html=True)
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"], unsafe_allow_html=True)

    if is_report_ready and st.session_state.messages and st.session_state.messages[-1]["role"] == "user":
        with st.chat_message("assistant"):
            with st.spinner("æ€è€ƒä¸­..."):
                st.session_state.conversation_context.chat_history = st.session_state.messages
                full_context_str = st.session_state.conversation_context.get_full_context_as_string()
                last_user_prompt = st.session_state.messages[-1]["content"]

                prompt_for_chat = f"""
ã‚ãªãŸã¯ã€ã™ã§ã«è¡Œã‚ã‚ŒãŸä¸€é€£ã®åˆ†æçµæœã‚’å®Œå…¨ã«ç†è§£ã—ãŸä¸Šã§ã€è¿½åŠ ã®è³ªå•ã«ç­”ãˆã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’è¸ã¾ãˆã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®æœ€å¾Œã®è³ªå•ã«ã€è©³ç´°ã‹ã¤çš„ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„ã€‚
å¿…è¦ã§ã‚ã‚Œã°ã€åˆ©ç”¨å¯èƒ½ãªDataFrameã‚’åˆ†æã—ã¦å›ç­”ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚

# ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
{full_context_str}

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®æœ€å¾Œã®è³ªå•
{last_user_prompt}

**ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã®æ„å›³ã‚’æ¨æ¸¬ã—ã¦ã€åˆ†æã®éç¨‹ã§ç”Ÿæˆã•ã‚ŒãŸDataFrameã‚’ã—ã£ã‹ã‚Šã¨ç¢ºèªã—ãŸã†ãˆã§è«–ç†çš„ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚**
"""
                all_dfs = {**st.session_state.initial_df_dict, **st.session_state.work_df_dict}
                chat_agent = create_pandas_dataframe_agent(
                    llm=ChatOpenAI(model="gpt-4.1", temperature=0, api_key=get_llm_client().api_key),
                    df=all_dfs,
                    agent_type="zero-shot-react-description",
                    verbose=True,
                    allow_dangerous_code=True,
                    return_intermediate_steps=True,
                    agent_executor_kwargs={"handle_parsing_errors": True},
                    df_exec_instruction=True,
                )

                st_callback_container = st.container()
                st_callback = StreamlitCallbackHandler(parent_container=st_callback_container, max_thought_containers=2, expand_new_thoughts=False)
                
                try:
                    response_dict = chat_agent.invoke({"input": prompt_for_chat}, {"callbacks": [st_callback]})
                    response_text = response_dict.get("output", "")
                except Exception as e:
                    response_text = f"ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
                    logger.error(f"Chat agent invocation failed: {e}")
                
                st.markdown(response_text, unsafe_allow_html=True)
                st.session_state.messages.append({"role": "assistant", "content": response_text})

    if is_report_ready:
        if chat_prompt := st.chat_input("ãƒ¬ãƒãƒ¼ãƒˆã‚„åˆ†æã«ã¤ã„ã¦è¿½åŠ ã§è³ªå•ã—ã¦ãã ã•ã„"):
            st.session_state.messages.append({"role": "user", "content": chat_prompt})
            st.rerun()
